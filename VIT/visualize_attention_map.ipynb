{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from VIT import VisionTransformer, CONFIGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Prepare Model\n",
    "config = CONFIGS[\"ViT-B_16\"]\n",
    "model = VisionTransformer(config, num_classes=200, zero_head=False, img_size=224, vis=True, extra_attention=None)\n",
    "# model.load_from(np.load(\"pretrain_weights/imagenet21k_ViT-B_16.npz\"))\n",
    "\n",
    "weights = torch.load(\"Tiny ImageNet_origin_12_checkpoint.bin\")\n",
    "\n",
    "model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "im = Image.open(\"visualization/test.JPEG\")\n",
    "x = transform(im)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (12) must match the size of tensor b (197) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# To account for residual connections, we add an identity matrix to the\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# attention matrix and re-normalize the weights.\u001b[39;00m\n\u001b[0;32m     10\u001b[0m residual_att \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39meye(att_mat\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m---> 11\u001b[0m aug_att_mat \u001b[38;5;241m=\u001b[39m \u001b[43matt_mat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresidual_att\u001b[49m\n\u001b[0;32m     12\u001b[0m aug_att_mat \u001b[38;5;241m=\u001b[39m aug_att_mat \u001b[38;5;241m/\u001b[39m aug_att_mat\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# Recursively multiply the weight matrices\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (12) must match the size of tensor b (197) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "logits, att_mat = model(x.unsqueeze(0))\n",
    "\n",
    "att_mat = torch.stack(att_mat).squeeze(1)\n",
    "\n",
    "# Average the attention weights across all heads.\n",
    "att_mat = torch.mean(att_mat, dim=1)\n",
    "\n",
    "# To account for residual connections, we add an identity matrix to the\n",
    "# attention matrix and re-normalize the weights.\n",
    "residual_att = torch.eye(att_mat.size(1))\n",
    "aug_att_mat = att_mat + residual_att\n",
    "aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n",
    "\n",
    "# Recursively multiply the weight matrices\n",
    "joint_attentions = torch.zeros(aug_att_mat.size())\n",
    "joint_attentions[0] = aug_att_mat[0]\n",
    "for n in range(1, aug_att_mat.size(0)):\n",
    "    joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n-1])\n",
    "    \n",
    "# Attention from the output token to the input space.\n",
    "v = joint_attentions[-1]\n",
    "grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n",
    "mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n",
    "\n",
    "mask_threshold = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "\n",
    "mask = np.power(mask, 2.5)   \n",
    "mask = (mask - mask.min()) / (mask.max() - mask.min())\n",
    "\n",
    "mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n",
    "result = (mask * im).astype(\"uint8\")\n",
    "\n",
    "# print(result.shape)\n",
    "\n",
    "threshold1 = 0.4\n",
    "binary_mask = (mask_threshold > threshold1).astype(np.uint8) * 255\n",
    "\n",
    "contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "im_marked1 = np.array(im.copy())  # 转换为NumPy数组\n",
    "cv2.drawContours(im_marked1, contours, -1, (0, 255, 0), 2) \n",
    "\n",
    "\n",
    "threshold2 = 0.6\n",
    "binary_mask = (mask_threshold > threshold2).astype(np.uint8) * 255\n",
    "\n",
    "contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "im_marked2 = np.array(im.copy())  # 转换为NumPy数组\n",
    "cv2.drawContours(im_marked2, contours, -1, (0, 255, 0), 2) \n",
    "\n",
    "\n",
    "threshold3 = 0.8\n",
    "binary_mask = (mask_threshold > threshold3).astype(np.uint8) * 255\n",
    "\n",
    "contours, _ = cv2.findContours(binary_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "im_marked3 = np.array(im.copy())  # 转换为NumPy数组\n",
    "cv2.drawContours(im_marked3, contours, -1, (0, 255, 0), 2) \n",
    "\n",
    "\n",
    "plt.imshow(im)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax2, ax3, ax4, ax5) = plt.subplots(ncols=4, figsize=(20, 20))\n",
    "\n",
    "# ax1.set_title('Input Image')\n",
    "ax2.set_title(f'Attention Outline(threshold={threshold1})')\n",
    "ax3.set_title(f'Attention Outline(threshold={threshold2})')\n",
    "ax4.set_title(f'Attention Outline(threshold={threshold3})')\n",
    "ax5.set_title('Attention Map * Input Image')\n",
    "# _ = ax1.imshow(im)\n",
    "_ = ax2.imshow(im_marked1)\n",
    "_ = ax3.imshow(im_marked2)\n",
    "_ = ax4.imshow(im_marked3)\n",
    "_ = ax5.imshow(im)\n",
    "\n",
    "ax5.imshow(im)\n",
    "heatmap = ax5.imshow(mask.squeeze(), cmap='jet', alpha=0.5)  # 半透明热力图叠加"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference\n",
    "* [attention_flow](https://github.com/samiraabnar/attention_flow)\n",
    "* [vit-keras](https://github.com/faustomorales/vit-keras)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
