{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd14232",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing\n",
    "import io\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "from VIT import VisionTransformer, CONFIGS\n",
    "from train import valid\n",
    "from utils.data_utils import get_loader\n",
    "\n",
    "from train import train, setup, set_seed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13de944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 224, 224])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = CONFIGS[\"ViT-B_16\"]\n",
    "model = VisionTransformer(config, num_classes=200, zero_head=False, img_size=224, vis=False, extra_attention=None)\n",
    "model.load_from(np.load(\"pretrain_weights/imagenet21k_ViT-B_16.npz\"))\n",
    "\n",
    "# weights = torch.load(\"pretrain_weights/imagenet21k_ViT-B_16.npz\")\n",
    "\n",
    "# model.load_state_dict(weights)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
    "])\n",
    "im = Image.open(\"visualization/test.JPEG\")\n",
    "x = transform(im)\n",
    "x.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0afec8df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    pass\n",
    "\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fc1a44cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "extra attention: GAU\n",
      "num of blocks: 12\n",
      "single block Parameter: 7.7M\n",
      "Total Parameter: 93.1M\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def args_set(args, dataset=\"cifar10\", extra_attention=None, num_blocks=12, train_example_nums=512, test_example_nums=128, batch_size=16, total_steps=500, warmup_steps=50, eval_every=64, learning_rate=3e-2):\n",
    "    args.extra_attention = extra_attention\n",
    "\n",
    "    args.num_blocks = num_blocks\n",
    "\n",
    "    args.train_example_nums = train_example_nums\n",
    "    args.test_example_nums = test_example_nums\n",
    "    \n",
    "    args.name = dataset + '_' + extra_attention + '_' + str(num_blocks) if extra_attention != None else dataset + '_' + 'origin' + '_' + str(num_blocks)\n",
    "    args.dataset = dataset\n",
    "    args.model_type = \"ViT-B_16\"\n",
    "    args.pretrained_dir = \"./pretrain_weights/imagenet21k_ViT-B_16.npz\"\n",
    "    args.output_dir = \"output\"\n",
    "\n",
    "    args.img_size = 224\n",
    "    args.train_batch_size = batch_size\n",
    "    args.eval_batch_size = batch_size\n",
    "    args.eval_every = eval_every\n",
    "\n",
    "    args.learning_rate = learning_rate\n",
    "    args.weight_decay = 0\n",
    "    args.num_steps = total_steps\n",
    "    args.decay_type = \"cosine\"\n",
    "    args.warmup_steps = warmup_steps\n",
    "    args.max_grad_norm = 1.0\n",
    "\n",
    "    args.local_rank = -1\n",
    "    args.seed = 42\n",
    "    args.gradient_accumulation_steps = 1\n",
    "    args.fp16 = False\n",
    "    args.fp16_opt_level = 'O2'\n",
    "    args.loss_scale = 0\n",
    "\n",
    "    return args\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dataset = 'Tiny ImageNet' #['cifar10', 'cifar100', 'Tiny ImageNet']\n",
    "\n",
    "# extra_attention = \"MoE\" #[None, 'Random', 'GAU', 'Global_SlidingWindow', 'BigBird', 'MoE', 'LowRank']\n",
    "extra_attention = 'GAU'\n",
    "num_blocks = 12\n",
    "\n",
    "train_example_nums = 100000 #训练数据量\n",
    "\n",
    "test_example_nums = 10000 #测试数据量\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "total_steps = 50000\n",
    "\n",
    "warmup_steps = 500\n",
    "\n",
    "eval_every = 500\n",
    "\n",
    "learning_rate = 3e-2\n",
    "\n",
    "\n",
    "\n",
    "args = args_set(args, dataset, extra_attention, num_blocks, train_example_nums, test_example_nums, batch_size, total_steps, warmup_steps, eval_every, learning_rate)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "args.n_gpu = torch.cuda.device_count()\n",
    "args.device = device\n",
    "\n",
    "set_seed(args)\n",
    "args, model = setup(args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ee6a262",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating... (loss=5.29832):  10%|| 61/625 [00:14<02:15,  4.15it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mvalid\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mget_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\港大\\sem2\\MATH7503\\Project\\VIT\\train.py:120\u001b[0m, in \u001b[0;36mvalid\u001b[1;34m(args, model, test_loader, global_step)\u001b[0m\n\u001b[0;32m    117\u001b[0m     logits \u001b[38;5;241m=\u001b[39m model(x)[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    119\u001b[0m     eval_loss \u001b[38;5;241m=\u001b[39m loss_fct(logits, y)\n\u001b[1;32m--> 120\u001b[0m     eval_losses\u001b[38;5;241m.\u001b[39mupdate(\u001b[43meval_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    122\u001b[0m     preds \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(logits, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(all_preds) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "valid(args, model, get_loader(args)[1], 10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepLearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
