{
 "cells": [
  {
   "cell_type": "code",
   "id": "02900dee-70d7-4327-a827-a3768658beb5",
   "metadata": {},
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import BertConfig\n",
    "from transformers.models.bert.modeling_bert import BertSelfAttention\n",
    "\n",
    "class MoEAttentionExpert(nn.Module):\n",
    "    \"\"\"单个Attention专家\"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        if config.hidden_size % config.num_attention_heads != 0:\n",
    "            raise ValueError(\n",
    "                f\"The hidden size ({config.hidden_size}) is not a multiple of the number of attention \"\n",
    "                f\"heads ({config.num_attention_heads})\"\n",
    "            )\n",
    "            \n",
    "        self.num_attention_heads = config.num_attention_heads\n",
    "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
    "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
    "\n",
    "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
    "\n",
    "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def forward(self, hidden_states, attention_mask=None, **kwargs):\n",
    "        mixed_query_layer = self.query(hidden_states)\n",
    "        mixed_key_layer = self.key(hidden_states)\n",
    "        mixed_value_layer = self.value(hidden_states)\n",
    "\n",
    "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
    "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
    "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
    "\n",
    "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
    "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
    "        \n",
    "        if attention_mask is not None:\n",
    "            attention_scores = attention_scores + attention_mask\n",
    "\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        attention_probs = self.dropout(attention_probs)\n",
    "\n",
    "        context_layer = torch.matmul(attention_probs, value_layer)\n",
    "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
    "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
    "        context_layer = context_layer.view(*new_context_layer_shape)\n",
    "        \n",
    "        return (context_layer,)\n",
    "\n",
    "class MoEAttention(BertSelfAttention):\n",
    "    \"\"\"混合专家Attention层\"\"\"\n",
    "    def __init__(self, config, num_experts=4, top_k=2):\n",
    "        super().__init__(config)\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        \n",
    "        # 创建专家池\n",
    "        self.experts = nn.ModuleList([MoEAttentionExpert(config) for _ in range(num_experts)])\n",
    "        \n",
    "        # 门控网络\n",
    "        self.gate = nn.Linear(config.hidden_size, num_experts)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        hidden_states,\n",
    "        attention_mask=None,\n",
    "        head_mask=None,\n",
    "        encoder_hidden_states=None,\n",
    "        encoder_attention_mask=None,\n",
    "        past_key_value=None,\n",
    "        output_attentions=False,\n",
    "    ):\n",
    "        # MoE Attention目前不支持encoder_hidden_states(即不支持cross-attention)\n",
    "        if encoder_hidden_states is not None:\n",
    "            raise ValueError(\"MoEAttention does not support cross-attention\")\n",
    "            \n",
    "        # 计算门控权重 - 使用[CLS] token或平均池化\n",
    "        gate_input = hidden_states[:, 0, :]  # 使用[CLS] token\n",
    "        # 或者: gate_input = hidden_states.mean(dim=1)  # 使用平均池化\n",
    "        \n",
    "        gate_logits = self.gate(gate_input)  # [batch_size, num_experts]\n",
    "        gate_probs = self.softmax(gate_logits)\n",
    "        \n",
    "        # 选择top-k专家\n",
    "        top_k_gate_probs, top_k_indices = torch.topk(gate_probs, self.top_k, dim=-1)\n",
    "        top_k_gate_probs = top_k_gate_probs / top_k_gate_probs.sum(dim=-1, keepdim=True)\n",
    "        \n",
    "        # 初始化输出\n",
    "        batch_size, seq_length, _ = hidden_states.shape\n",
    "        context_layer = torch.zeros(\n",
    "            (batch_size, seq_length, self.all_head_size),\n",
    "            dtype=hidden_states.dtype,\n",
    "            device=hidden_states.device\n",
    "        )\n",
    "        \n",
    "        # 如果需要输出attention权重\n",
    "        all_attentions = () if output_attentions else None\n",
    "        \n",
    "        # 计算各专家的输出并加权组合\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # 创建当前专家的mask\n",
    "            expert_mask = (top_k_indices == i).any(dim=1).float()  # [batch_size]\n",
    "            \n",
    "            if expert_mask.sum() == 0:\n",
    "                continue\n",
    "                \n",
    "            # 计算当前专家输出\n",
    "            expert_output = expert(\n",
    "                hidden_states=hidden_states,\n",
    "                attention_mask=attention_mask\n",
    "            )[0]  # 取第一个输出(忽略可能的attention probs)\n",
    "            \n",
    "            # 计算权重 (batch_size, 1, 1)\n",
    "            weights = (top_k_gate_probs * (top_k_indices == i).float()).sum(dim=1)\n",
    "            weights = weights.view(-1, 1, 1)\n",
    "            \n",
    "            # 只对选中的batch应用该专家的输出\n",
    "            context_layer += expert_output * weights * expert_mask.view(-1, 1, 1)\n",
    "        \n",
    "        # 处理head mask(如果需要)\n",
    "        if head_mask is not None:\n",
    "            context_layer = context_layer * head_mask\n",
    "            \n",
    "        # 返回格式与原始BERT一致\n",
    "        outputs = (context_layer,)\n",
    "        if output_attentions:\n",
    "            outputs += (all_attentions,)\n",
    "            \n",
    "        return outputs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fd62ca4-f867-4d37-a259-d5e378983a42",
   "metadata": {},
   "source": [
    "from transformers import BertPreTrainedModel, BertModel\n",
    "\n",
    "class BertWithMoEAttention(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.config = config\n",
    "        \n",
    "        # 原始BERT模型\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # 替换所有attention层为MoE Attention\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            layer.attention.self = MoEAttention(config, num_experts=4, top_k=2)\n",
    "    \n",
    "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, **kwargs):\n",
    "        return self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            **kwargs\n",
    "        )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "be93be35-f7ae-40f1-8bd0-bc99c72626d1",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# 初始化模型\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertWithMoEAttention(config)\n",
    "\n",
    "# 使用示例输入\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\")\n",
    "\n",
    "# 前向传播\n",
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e91aa002-3525-4dea-b3df-bcfeeb85627b",
   "metadata": {},
   "source": [
    "from transformers import BertForPreTraining\n",
    "\n",
    "# 初始化自定义模型\n",
    "config = BertConfig.from_pretrained(\"bert-base-uncased\")\n",
    "model = BertWithMoEAttention(config)\n",
    "\n",
    "# 加载原始BERT预训练权重\n",
    "pretrained_model = BertForPreTraining.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# 获取原始BERT的状态字典\n",
    "pretrained_state_dict = pretrained_model.state_dict()\n",
    "\n",
    "# 获取自定义模型的状态字典\n",
    "model_state_dict = model.state_dict()\n",
    "\n",
    "# 筛选可加载的权重（排除自定义Attention部分的权重）\n",
    "loadable_weights = {k: v for k, v in pretrained_state_dict.items() \n",
    "                   if k in model_state_dict and \"attention.self.experts\" not in k}\n",
    "\n",
    "# 加载可用的预训练权重\n",
    "model_state_dict.update(loadable_weights)\n",
    "model.load_state_dict(model_state_dict)\n",
    "\n",
    "print(\"部分预训练权重加载完成（跳过自定义Attention部分）\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c19a43f6-2553-4b7f-94ab-62976bfccade",
   "metadata": {},
   "source": [
    "outputs = model(**inputs)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "00c6f9f1-43fb-468d-80e1-fee6c31fb35e",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "model.to(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "daf1bf52-4e11-443a-a183-81f0dfd7f7cc",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# 加载SST-2数据集\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# 查看数据集结构\n",
    "print(dataset)\n",
    "print(\"\\n样例:\")\n",
    "print(dataset[\"train\"][0])\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"sentence\"], \n",
    "        truncation=True, \n",
    "        padding=\"max_length\", \n",
    "        max_length=128\n",
    "    )\n",
    "\n",
    "# 预处理数据集\n",
    "encoded_dataset = dataset.map(preprocess_function, batched=True)\n",
    "encoded_dataset = encoded_dataset.rename_column(\"label\", \"labels\")\n",
    "encoded_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bc5f71ad-0448-49d7-8b63-0f7fe4eb5156",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, model_name=\"model\"):\n",
    "    # 准备DataLoader\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset.select(range(5000)),  # 使用前5000个样本作为小数据集\n",
    "        batch_size=16,\n",
    "        shuffle=True\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_dataset.select(range(872)),  # 使用前1000个样本验证\n",
    "        batch_size=16\n",
    "    )\n",
    "    \n",
    "    # 优化器和学习率调度\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "    num_epochs = 3\n",
    "    num_training_steps = num_epochs * len(train_dataloader)\n",
    "    lr_scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_training_steps\n",
    "    )\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}\")\n",
    "        \n",
    "        for batch in progress_bar:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({\"loss\": loss.item()})\n",
    "        \n",
    "        avg_train_loss = total_loss / len(train_dataloader)\n",
    "        \n",
    "        # 验证\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch in val_dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            with torch.no_grad():\n",
    "                outputs = model(**batch)\n",
    "            \n",
    "            val_loss += outputs.loss.item()\n",
    "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "            correct += (predictions == batch[\"labels\"]).sum().item()\n",
    "            total += len(batch[\"labels\"])\n",
    "        \n",
    "        val_accuracy = correct / total\n",
    "        avg_val_loss = val_loss / len(val_dataloader)\n",
    "        \n",
    "        print(f\"{model_name} - Epoch {epoch+1}:\")\n",
    "        print(f\"  Train Loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val Loss: {avg_val_loss:.4f}\")\n",
    "        print(f\"  Val Acc: {val_accuracy:.4f}\\n\")\n",
    "    \n",
    "    return model"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7c72648a-3bef-46b3-8008-f298766da028",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "# 训练原始BERT模型\n",
    "print(\"=== 训练原始BERT模型 ===\")\n",
    "original_bert = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2\n",
    ")\n",
    "train_model(original_bert, encoded_dataset[\"train\"], encoded_dataset[\"validation\"], \"Original BERT\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f649b831-0837-4c28-892b-78b01912a99f",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "from transformers import BertPreTrainedModel, BertModel, BertConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "\n",
    "class BertWithMoEAttentionForSequenceClassification(BertPreTrainedModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.config = config\n",
    "        \n",
    "        # 原始BERT模型（不包含分类头）\n",
    "        self.bert = BertModel(config)\n",
    "        \n",
    "        # 替换所有attention层为MoE Attention\n",
    "        for layer in self.bert.encoder.layer:\n",
    "            layer.attention.self = MoEAttention(config, num_experts=4, top_k=2)\n",
    "        \n",
    "        # 分类器\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
    "        \n",
    "        # 初始化权重\n",
    "        self.post_init()\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids=None,\n",
    "        attention_mask=None,\n",
    "        token_type_ids=None,\n",
    "        position_ids=None,\n",
    "        head_mask=None,\n",
    "        inputs_embeds=None,\n",
    "        labels=None,\n",
    "        output_attentions=None,\n",
    "        output_hidden_states=None,\n",
    "        return_dict=None,\n",
    "    ):\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "        \n",
    "        # 通过BERT模型获取输出\n",
    "        outputs = self.bert(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids,\n",
    "            position_ids=position_ids,\n",
    "            head_mask=head_mask,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        \n",
    "        # 获取[CLS] token的隐藏状态\n",
    "        pooled_output = outputs[1]\n",
    "        \n",
    "        # 分类头\n",
    "        pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(pooled_output)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
    "        \n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[2:]\n",
    "            return ((loss,) + output) if loss is not None else output\n",
    "        \n",
    "        return SequenceClassifierOutput(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "# 训练自定义MoE Attention BERT模型\n",
    "print(\"\\n=== 训练自定义MoE Attention BERT模型 ===\")\n",
    "moe_bert = BertWithMoEAttentionForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", \n",
    "    num_labels=2\n",
    ")\n",
    "train_model(moe_bert, encoded_dataset[\"train\"], encoded_dataset[\"validation\"], \"MoE BERT\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "57f9c5b7-7add-4e8f-bdec-74ede8b565fc",
   "metadata": {},
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_results(original_results, moe_results):\n",
    "    epochs = range(1, len(original_results[\"train_loss\"]) + 1)\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # 训练损失对比\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, original_results[\"train_loss\"], label=\"Original BERT\")\n",
    "    plt.plot(epochs, moe_results[\"train_loss\"], label=\"MoE BERT\")\n",
    "    plt.title(\"Training Loss Comparison\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # 验证准确率对比\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, original_results[\"val_acc\"], label=\"Original BERT\")\n",
    "    plt.plot(epochs, moe_results[\"val_acc\"], label=\"MoE BERT\")\n",
    "    plt.title(\"Validation Accuracy Comparison\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 假设我们记录了训练过程中的指标\n",
    "example_results = {\n",
    "    \"original\": {\n",
    "        \"train_loss\": [0.45, 0.32, 0.28],\n",
    "        \"val_acc\": [0.85, 0.87, 0.88]\n",
    "    },\n",
    "    \"moe\": {\n",
    "        \"train_loss\": [0.43, 0.30, 0.25],\n",
    "        \"val_acc\": [0.86, 0.88, 0.89]\n",
    "    }\n",
    "}\n",
    "\n",
    "plot_results(example_results[\"original\"], example_results[\"moe\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "94334f02-6fc5-42e8-b2a2-dfdc30810a03",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "test"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
